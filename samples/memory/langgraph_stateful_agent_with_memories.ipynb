{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2f5e94-92be-4358-ba5a-33a0009b46d0",
   "metadata": {},
   "source": [
    "# LangGraph Agent with Bedrock AgentCore Memories\n",
    "\n",
    "This notebook walks through creating a simple LangGraph agent with a chatbot (llm) node and a tool calling node. \n",
    "\n",
    "This agent integrates with Amazon Bedrock Agentcore Memory to store messages so that the agent can pick back up where it left off if the conversation is interrupted. \n",
    "\n",
    "Before and after each time the LLM node is invoked, the previous `User` message and the generated `LLM` message are saved to Bedrock Agentcore Memory. If the session is interuptted, the agent detects that no messages are present in the state, then lists the past 10 events from the session from Agentcore Memory. These messages are loaded into the state so the agent can have the previous context.\n",
    "\n",
    "### Pre-requisites for this sample\n",
    "- Amazon Web Services account\n",
    "- Amazon Bedrock Agentcore Memory configured - https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-getting-started.html#memory-getting-started-create-memory\n",
    "- Amazon Bedrock model access - https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51ead6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "from langchain_aws.memory.bedrock_agentcore import (\n",
    "    store_agentcore_memory_events,\n",
    "    list_agentcore_memory_events,\n",
    ")\n",
    "\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "memory = InMemorySaver()\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    max_tokens=5000,\n",
    "    region_name=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c8a58-b1ec-43e8-8103-fab8ad85e422",
   "metadata": {},
   "source": [
    "## Configure Agentcore Memory\n",
    "\n",
    "AgentCore short-term memories are organized by a Memory ID (overall memory store) and then categorized by Actor ID (i.e which user) and Session ID (which chat session). \n",
    "\n",
    "Long term memories are stored in namespaces. Please see the long term memory search example notebook for more information. By configuring different strategies (i.e. Summarization, User Preferences) these short term memories are processed async as long term memories in the specified namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d189d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west-2\"\n",
    "MEMORY_ID = \"YOUR_MEMORY_ID\"\n",
    "SESSION_ID = \"session-10\"\n",
    "ACTOR_ID = \"user-1\"\n",
    "\n",
    "# Initialize the memory client\n",
    "memory_client = MemoryClient(region_name=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfdcff5-19c2-4b68-a371-9cadb70907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "memory = InMemorySaver()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189596c2-f725-4c11-bb59-8e58ceed51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820010bb-6c4e-4911-aeea-e7aa4b496109",
   "metadata": {},
   "source": [
    "## Pre/Post Model Hooks\n",
    "\n",
    "For this implementation, short term memories (message events) are stored before and after model invocation. Before the model runs, the previous user message is saved. After the model runs, the LLM message is saved. \n",
    "\n",
    "If the conversation is just starting or no messages are present in the state, a check is done in Agentcore memory to see if there are any previous events in that actor/session combination. If so, those messages are added as a special System message so that the LLM has the context from the previous interrupted conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17dc78a-8f7a-476c-9399-b97f6d9357b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model_hook(state):\n",
    "    \"\"\"Store the previous user or tool message before the LLM responds, or load recent history if state is empty\"\"\"\n",
    "    try:\n",
    "        # Check if there are no messages in the state\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if len(state[\"messages\"]) == 1:\n",
    "            # Load the last 10 events from memory\n",
    "            recent_messages = list_agentcore_memory_events(\n",
    "                memory_client, MEMORY_ID, ACTOR_ID, SESSION_ID, 10\n",
    "            )\n",
    "            print(\"No messages in history, attempting to load from AgentCore memory\")\n",
    "            \n",
    "            if recent_messages:\n",
    "                print(f\"{len(recent_messages)} recent messages found in AgentCore memory, loading them into context.\")\n",
    "\n",
    "                context_content = \"No messages in history, attempting to load from AgentCore memory\"\n",
    "                for i, msg in enumerate(recent_messages, 1):\n",
    "                    # Extract message details\n",
    "                    if hasattr(msg, 'content'):\n",
    "                        content = msg.content\n",
    "                        role = getattr(msg, 'type', 'unknown')\n",
    "                    elif isinstance(msg, dict):\n",
    "                        content = msg.get('content', str(msg))\n",
    "                        role = msg.get('role', msg.get('type', 'unknown'))\n",
    "                    else:\n",
    "                        content = str(msg)\n",
    "                        role = 'unknown'\n",
    "                    \n",
    "                    context_content += f\"\"\"\n",
    "                Message {i}:\n",
    "                Role: {role.capitalize()}\n",
    "                Content: {content}\n",
    "                ---\"\"\"\n",
    "                \n",
    "                context_content += \"\"\"\n",
    "                \n",
    "                === END HISTORY ===\n",
    "                \n",
    "                Please use this context to continue our conversation naturally. You should reference relevant parts of this history when appropriate, but don't explicitly mention that you're loading from memory unless asked.\n",
    "                \"\"\"\n",
    "                # Create a special system message with the previous context\n",
    "                ai_context_msg = SystemMessage(content=context_content)\n",
    "                state['messages'] = [ai_context_msg] + [last_message]\n",
    "                return state\n",
    "            else:\n",
    "                print(\"No past agentcore messages found.\")\n",
    "\n",
    "        # Store the last message (user or tool message) as before\n",
    "        print(f\"Storing event pre-model: {last_message}\")\n",
    "        store_agentcore_memory_events(\n",
    "            memory_client=memory_client,\n",
    "            memory_id=MEMORY_ID,\n",
    "            actor_id=ACTOR_ID,\n",
    "            session_id=SESSION_ID,\n",
    "            messages=[last_message]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Memory operation failed: {e}\")\n",
    "    \n",
    "    return state  # Return state unchanged if messages exist or if there was an error\n",
    "\n",
    "\n",
    "\n",
    "def post_model_hook(state):\n",
    "    \"\"\"Store the LLM response after it's generated\"\"\"\n",
    "    try:\n",
    "        # Get the last message (LLM response)\n",
    "        if state[\"messages\"]:\n",
    "            last_message = state[\"messages\"][-1]\n",
    "            print(f\"Storing event post-model: {last_message}\")\n",
    "            store_agentcore_memory_events(\n",
    "                memory_client=memory_client,\n",
    "                memory_id=MEMORY_ID,\n",
    "                actor_id=ACTOR_ID,\n",
    "                session_id=SESSION_ID,\n",
    "                messages=[last_message]\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Memory storage failed: {e}\")\n",
    "\n",
    "\n",
    "def chatbot_with_hooks(state: State):\n",
    "    # Pre-hook: Store the incoming user/tool message OR load recent history if empty\n",
    "    modified_state = pre_model_hook(state)\n",
    "    \n",
    "    # LLM call\n",
    "    response = llm_with_tools.invoke(modified_state[\"messages\"])\n",
    "    \n",
    "    # Create the new state with all messages including the response\n",
    "    new_state = {\"messages\": modified_state[\"messages\"] + [response]}\n",
    "    \n",
    "    # Post-hook: Store the LLM response\n",
    "    post_model_hook(new_state)\n",
    "    \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8022ef56-6f71-4bf8-9bf3-a9ac254f6b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x10bbb7640>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Build graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_hooks)\n",
    "\n",
    "tool_node = ToolNode(tools=[multiply])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\", tools_condition, {\"tools\": \"tools\", \"__end__\": \"__end__\"}\n",
    ")\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77465a6-1005-402a-b911-cbde002b6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to invoke the chatbot\n",
    "def chat(user_input: str):\n",
    "    \"\"\"Send a message to the chatbot and display the response\"\"\"\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    \n",
    "    for event in events:\n",
    "        # Print the last message (which will be the response)\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b02dd91-a304-457b-b308-0bfa9d6ec8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, world!\n",
      "No messages in history, attempting to load from AgentCore memory\n",
      "No past agentcore messages found.\n",
      "Storing event pre-model: content='Hello, world!' additional_kwargs={} response_metadata={} id='68931e20-11b2-423d-bbef-d4a1a7508ec1'\n",
      "Storing event post-model: content=\"Hello! Welcome! How can I assist you today? I have a function available that can multiply two numbers. Would you like me to help you with a multiplication calculation? If so, please provide me with the two numbers you'd like to multiply.\" additional_kwargs={} response_metadata={'ResponseMetadata': {'RequestId': '1280187f-a22d-4ec8-97d6-74b84b42437d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 29 Aug 2025 21:39:47 GMT', 'content-type': 'application/json', 'content-length': '531', 'connection': 'keep-alive', 'x-amzn-requestid': '1280187f-a22d-4ec8-97d6-74b84b42437d'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [2574]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'} id='run--76244606-9264-45c1-b380-7afe009e2170-0' usage_metadata={'input_tokens': 387, 'output_tokens': 53, 'total_tokens': 440, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! Welcome! How can I assist you today? I have a function available that can multiply two numbers. Would you like me to help you with a multiplication calculation? If so, please provide me with the two numbers you'd like to multiply.\n"
     ]
    }
   ],
   "source": [
    "chat(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80436df7-d4c9-41a2-bff7-37cded3e4aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Hello! Welcome! How can I assist you today? I have a function available that can multiply two numbers. Would you like me to help you with a multiplication calculation? If so, please provide me with the two numbers you'd like to multiply.\", additional_kwargs={'event_id': '0000001756503587000#f06f3cff'}, response_metadata={}),\n",
       " HumanMessage(content='Hello, world!', additional_kwargs={'event_id': '0000001756503584000#5a553d5c'}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_agentcore_memory_events(memory_client, MEMORY_ID, ACTOR_ID, SESSION_ID, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a14359-2fd6-45c2-a7a0-a5977c258dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the result of 1337 multiplied by 78?\n",
      "Storing event pre-model: content=\"What's the result of 1337 multiplied by 78?\" additional_kwargs={} response_metadata={} id='d81510bb-c9a8-474a-9264-0d3e87bbd2fd'\n",
      "Storing event post-model: content=[{'type': 'text', 'text': \"I'll calculate the result of 1337 multiplied by 78 for you.\"}, {'type': 'tool_use', 'name': 'multiply', 'input': {'a': 1337, 'b': 78}, 'id': 'tooluse_7ku3y30NS1-Vb-99T2r1Fw'}] additional_kwargs={} response_metadata={'ResponseMetadata': {'RequestId': 'b813c905-eabd-46f7-901d-7f30daead916', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 29 Aug 2025 21:40:00 GMT', 'content-type': 'application/json', 'content-length': '456', 'connection': 'keep-alive', 'x-amzn-requestid': 'b813c905-eabd-46f7-901d-7f30daead916'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': [2296]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'} id='run--9ca724ca-8c14-46b3-9236-34a075737ed0-0' tool_calls=[{'name': 'multiply', 'args': {'a': 1337, 'b': 78}, 'id': 'tooluse_7ku3y30NS1-Vb-99T2r1Fw', 'type': 'tool_call'}] usage_metadata={'input_tokens': 458, 'output_tokens': 89, 'total_tokens': 547, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'type': 'text', 'text': \"I'll calculate the result of 1337 multiplied by 78 for you.\"}, {'type': 'tool_use', 'name': 'multiply', 'input': {'a': 1337, 'b': 78}, 'id': 'tooluse_7ku3y30NS1-Vb-99T2r1Fw'}]\n",
      "Tool Calls:\n",
      "  multiply (tooluse_7ku3y30NS1-Vb-99T2r1Fw)\n",
      " Call ID: tooluse_7ku3y30NS1-Vb-99T2r1Fw\n",
      "  Args:\n",
      "    a: 1337\n",
      "    b: 78\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "104286\n",
      "Storing event pre-model: content='104286' name='multiply' id='656db2b6-f80e-42a6-8d96-f9da85d6a4d3' tool_call_id='tooluse_7ku3y30NS1-Vb-99T2r1Fw'\n",
      "Storing event post-model: content='The result of 1337 multiplied by 78 equals 104,286.' additional_kwargs={} response_metadata={'ResponseMetadata': {'RequestId': '494b7af6-139d-4ef9-ae37-e0ff5f12700e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 29 Aug 2025 21:40:01 GMT', 'content-type': 'application/json', 'content-length': '344', 'connection': 'keep-alive', 'x-amzn-requestid': '494b7af6-139d-4ef9-ae37-e0ff5f12700e'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [996]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'} id='run--ab7051b2-cb8c-4e9f-b931-34b1af1944b7-0' usage_metadata={'input_tokens': 560, 'output_tokens': 23, 'total_tokens': 583, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of 1337 multiplied by 78 equals 104,286.\n"
     ]
    }
   ],
   "source": [
    "chat(\"What's the result of 1337 multiplied by 78?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e910ec4-9757-4ed8-bdcd-aef2461c25fa",
   "metadata": {},
   "source": [
    "## Clearing and loading the previous memories\n",
    "\n",
    "For this sample, we will clear the in-memory state so that no messages are present. The messages from the conversation will be loaded from Bedrock Agentcore memory so that the agent can continue where the user left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd5a83e-c1be-478e-aac0-2f0b6f5297f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory(memory, thread_id: str) -> None:\n",
    "    \"\"\" Clear the memory for a given thread_id. \"\"\"\n",
    "    try:\n",
    "        # If it's an InMemorySaver (which MemorySaver is an alias for),\n",
    "        # we can directly clear the storage and writes\n",
    "        if hasattr(memory, 'storage') and hasattr(memory, 'writes'):\n",
    "            # Clear all checkpoints for this thread_id (all namespaces)\n",
    "            memory.storage.pop(thread_id, None)\n",
    "\n",
    "            # Clear all writes for this thread_id (for all namespaces)\n",
    "            keys_to_remove = [key for key in memory.writes.keys() if key[0] == thread_id]\n",
    "            for key in keys_to_remove:\n",
    "                memory.writes.pop(key, None)\n",
    "\n",
    "            print(f\"Memory cleared for thread_id: {thread_id}\")\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing InMemorySaver storage for thread_id {thread_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011f536e-f9bc-4a18-be8a-39d758e003e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: 1\n",
      "✅ No conversation state found - memory is clear!\n"
     ]
    }
   ],
   "source": [
    "# Clear the in-memory messages\n",
    "thread_id = config.get(\"configurable\").get(\"thread_id\")\n",
    "print(f\"Thread ID: {thread_id}\")\n",
    "memory.delete_thread(config.get(\"configurable\").get(\"thread_id\"))\n",
    "\n",
    "state = graph.get_state(config)\n",
    "        \n",
    "if state and state.values and 'messages' in state.values:\n",
    "    messages = state.values['messages']\n",
    "    print(f\"📝 Found {len(messages)} messages in conversation state:\")\n",
    "    for i, msg in enumerate(messages):\n",
    "        print(f\"  {i+1}. {msg.type}: {msg.content[:100]}...\")\n",
    "else:\n",
    "    print(\"✅ No conversation state found - memory is clear!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99a2c72-d83f-4746-b0ee-bb902017f466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What numbers was I multiplying earlier?\n",
      "No messages in history, attempting to load from AgentCore memory\n",
      "6 recent messages found in AgentCore memory, loading them into context.\n",
      "Storing event post-model: content='Based on our earlier conversation, you were asking about multiplying 1337 by 78. I calculated that for you, and the result was 104,286.' additional_kwargs={} response_metadata={'ResponseMetadata': {'RequestId': 'b2504f9f-0c3b-494d-b478-41cc44bca118', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 29 Aug 2025 21:40:12 GMT', 'content-type': 'application/json', 'content-length': '429', 'connection': 'keep-alive', 'x-amzn-requestid': 'b2504f9f-0c3b-494d-b478-41cc44bca118'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [1470]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'} id='run--8f0bb0ac-588a-479d-aaa9-7392329cb1b6-0' usage_metadata={'input_tokens': 675, 'output_tokens': 38, 'total_tokens': 713, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on our earlier conversation, you were asking about multiplying 1337 by 78. I calculated that for you, and the result was 104,286.\n"
     ]
    }
   ],
   "source": [
    "chat(\"What numbers was I multiplying earlier?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e8d9f1e-7ac0-4099-994b-49da28f405e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What numbers was I multiplying earlier?', additional_kwargs={}, response_metadata={}, id='012b3c53-725c-4588-a0d4-46489bf80fdf'),\n",
       " SystemMessage(content=\"No messages in history, attempting to load from AgentCore memory\\n                Message 1:\\n                Role: Ai\\n                Content: The result of 1337 multiplied by 78 equals 104,286.\\n                ---\\n                Message 2:\\n                Role: Ai\\n                Content: I'll calculate the result of 1337 multiplied by 78 for you.\\n                ---\\n                Message 3:\\n                Role: Tool\\n                Content: 104286\\n                ---\\n                Message 4:\\n                Role: Human\\n                Content: What's the result of 1337 multiplied by 78?\\n                ---\\n                Message 5:\\n                Role: Ai\\n                Content: Hello! Welcome! How can I assist you today? I have a function available that can multiply two numbers. Would you like me to help you with a multiplication calculation? If so, please provide me with the two numbers you'd like to multiply.\\n                ---\\n                Message 6:\\n                Role: Human\\n                Content: Hello, world!\\n                ---\\n                \\n                === END HISTORY ===\\n                \\n                Please use this context to continue our conversation naturally. You should reference relevant parts of this history when appropriate, but don't explicitly mention that you're loading from memory unless asked.\\n                \", additional_kwargs={}, response_metadata={}, id='07cc8cda-0e33-4a83-88eb-3d429444b283'),\n",
       " AIMessage(content='Based on our earlier conversation, you were asking about multiplying 1337 by 78. I calculated that for you, and the result was 104,286.', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'b2504f9f-0c3b-494d-b478-41cc44bca118', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 29 Aug 2025 21:40:12 GMT', 'content-type': 'application/json', 'content-length': '429', 'connection': 'keep-alive', 'x-amzn-requestid': 'b2504f9f-0c3b-494d-b478-41cc44bca118'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [1470]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}, id='run--8f0bb0ac-588a-479d-aaa9-7392329cb1b6-0', usage_metadata={'input_tokens': 675, 'output_tokens': 38, 'total_tokens': 713, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = graph.get_state(config)\n",
    "state.values[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe5d85-b047-4143-a57c-93f7de7ba3d2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As you can see, Bedrock Agentcore memories can be saved and loaded easily using the short term memory API and the helper functions implemented in hooks. \n",
    "\n",
    "This is not a one-size fits all approach and developers can utilize the storing/listing/searching memory functionalities in their own node, pre/post model hooks, or as tools themselves. Check out the other examples for various implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3aca3-9308-4524-bd64-df6262a8831d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
